{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "Loading ckpt pretrained_encoder.pth\n",
      "DONE\n",
      "\n",
      "Loading ckpt pretrained_decoder.pth\n",
      "DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "import vis_utils_layout as vis_utils\n",
    "from data_layout import LayoutDataset, Tree\n",
    "import model_layout as model\n",
    "import tqdm\n",
    "\n",
    "sys.setrecursionlimit(5000) \n",
    "\n",
    "num_gen = 300\n",
    "num_recon = 200\n",
    "exp = 'magazine_2.5K'\n",
    "category = 'magazine'\n",
    "checkpoint = ''\n",
    "device = 'cuda:0'\n",
    "\n",
    "path = '/home/weiran/Projects/RvNN-Layout/GT-Layout/' + category + '/logs/' + exp\n",
    "data_path = '/home/weiran/Projects/RvNN-Layout/data/' + category + '-ours/' + exp\n",
    "\n",
    "# load train config\n",
    "conf = torch.load(path + '/conf.pth')\n",
    "\n",
    "# load object category information\n",
    "Tree.load_category_info(conf.category)\n",
    "\n",
    "# set up device\n",
    "print(f'Using device: {conf.device}')\n",
    "\n",
    "recon_dir = path + '/recon-test/'\n",
    "gen_dir = path + '/generation/'\n",
    "\n",
    "if os.path.exists(recon_dir):\n",
    "    shutil.rmtree(recon_dir)\n",
    "os.mkdir(recon_dir)\n",
    "\n",
    "if os.path.exists(gen_dir):\n",
    "    shutil.rmtree(gen_dir)\n",
    "os.mkdir(gen_dir)\n",
    "\n",
    "# create models\n",
    "# we disable probabilistic because we do not need to jitter the decoded z during inference\n",
    "encoder = model.RecursiveEncoder(conf, variational=True, probabilistic=False)\n",
    "decoder = model.RecursiveDecoder(conf)\n",
    "\n",
    "# load the pretrained models\n",
    "print('Loading ckpt pretrained_encoder.pth')\n",
    "data_to_restore = torch.load(path + '/ckpts/' + checkpoint + 'net_encoder.pth')\n",
    "encoder.load_state_dict(data_to_restore, strict=True)\n",
    "print('DONE\\n')\n",
    "print('Loading ckpt pretrained_decoder.pth')\n",
    "data_to_restore = torch.load(path + '/ckpts/' + checkpoint + 'net_decoder.pth')\n",
    "decoder.load_state_dict(data_to_restore, strict=True)\n",
    "print('DONE\\n')\n",
    "\n",
    "# send to device\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# set models to evaluation mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# read test.txt\n",
    "with open(data_path + '/test.txt', 'r') as fin:\n",
    "    data_list = [l.rstrip() for l in fin.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 vertical_branch    {1} 1 Box([0.0, 0.0, 0.6777777671813965, 0.9111111164093018])\n",
      "  ├0 title [LEAF] {2} 7 Box([0.05000000074505806, 0.0, 0.5444444417953491, 0.17777776718139648])\n",
      "  ├1 stack_branch    {3} 3 Box([0.0, 0.23333331942558289, 0.6777777671813965, 0.6777777671813965])\n",
      "  |  ├0 horizontal_branch    {4} 2 Box([0.1666666716337204, 0.28333333134651184, 0.3611111044883728, 0.5277777910232544])\n",
      "  |  |  ├0 vertical_branch    {5} 1 Box([0.1666666716337204, 0.28333333134651184, 0.16111111640930176, 0.5277777910232544])\n",
      "  |  |  |  ├0 text [LEAF] {7} 5 Box([0.1666666716337204, 0.28333333134651184, 0.16111111640930176, 0.17777779698371887])\n",
      "  |  |  |  ├1 text [LEAF] {8} 5 Box([0.1666666716337204, 0.4833333492279053, 0.16111111640930176, 0.21111111342906952])\n",
      "  |  |  |  ├2 text [LEAF] {9} 5 Box([0.1666666716337204, 0.7166666984558105, 0.16111111640930176, 0.09444444626569748])\n",
      "  |  |  ├1 vertical_branch    {6} 1 Box([0.3500000238418579, 0.36666667461395264, 0.17777778208255768, 0.22777777910232544])\n",
      "  |  |  |  ├0 text [LEAF] {10} 5 Box([0.3500000238418579, 0.36666667461395264, 0.17777778208255768, 0.06111111491918564])\n",
      "  |  |  |  ├1 text [LEAF] {11} 5 Box([0.3500000238418579, 0.45000001788139343, 0.17777778208255768, 0.14444445073604584])\n",
      "  |  ├1 image [LEAF] {3} 4 Box([0.0, 0.23333331942558289, 0.6777777671813965, 0.6777777671813965])\n",
      "\n",
      "0 vertical_branch    {0} 1 Box([0.0004451386630535126, 0.00019883151981048286, 0.6646232008934021, 0.9012202024459839])\n",
      "  ├0 title [LEAF] {0} 7 Box([0.013018541969358921, 0.0002704592188820243, 0.5608871579170227, 0.11164610832929611])\n",
      "  ├1 stack_branch    {0} 3 Box([0.0004471639113035053, 0.16494466364383698, 0.6644073724746704, 0.713090717792511])\n",
      "  |  ├0 horizontal_branch    {0} 2 Box([0.1513233184814453, 0.20810413360595703, 0.3877890706062317, 0.563611626625061])\n",
      "  |  |  ├0 vertical_branch    {0} 1 Box([0.15174874663352966, 0.20827756822109222, 0.16164179146289825, 0.5615292191505432])\n",
      "  |  |  |  ├0 text [LEAF] {0} 5 Box([0.1519635170698166, 0.20853562653064728, 0.15849915146827698, 0.21439379453659058])\n",
      "  |  |  |  ├1 text [LEAF] {0} 5 Box([0.15196773409843445, 0.44175413250923157, 0.15862782299518585, 0.21731939911842346])\n",
      "  |  |  |  ├2 text [LEAF] {0} 5 Box([0.15188100934028625, 0.7065714597702026, 0.1609681099653244, 0.11178100854158401])\n",
      "  |  |  ├1 vertical_branch    {0} 1 Box([0.34223294258117676, 0.2786867320537567, 0.18639671802520752, 0.2747265696525574])\n",
      "  |  |  |  ├0 text [LEAF] {0} 5 Box([0.3427462577819824, 0.27892544865608215, 0.1835355907678604, 0.09593532234430313])\n",
      "  |  |  |  ├1 text [LEAF] {0} 5 Box([0.34239038825035095, 0.40112727880477905, 0.18473045527935028, 0.1869012862443924])\n",
      "  |  ├1 image [LEAF] {0} 4 Box([0.0004471639113035053, 0.16494466364383698, 0.6644073724746704, 0.713090717792511])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "\n",
    "# load the gt data as the input\n",
    "obj = LayoutDataset.load_object(data_path + '/' + data_list[i] + '.json')\n",
    "obj.to(device)\n",
    "\n",
    "# feed through the encoder to get a code z\n",
    "# root_code = encoder.encode_structure(obj)\n",
    "root_code_and_kld = encoder.encode_structure(obj)\n",
    "root_code = root_code_and_kld[:, :conf.feature_size]\n",
    "\n",
    "# infer through the decoder to get the reconstructed output\n",
    "# set maximal tree depth to conf.max_tree_depth\n",
    "obj_arr = decoder.decode_structure(z=root_code, max_depth=conf.max_tree_depth)\n",
    "obj_arr.get_arrbox()\n",
    "\n",
    "# print(obj)\n",
    "obj.get_arrbox()\n",
    "print(obj)\n",
    "print(obj_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:19<00:00,  2.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# reconstruct shapes\n",
    "with torch.no_grad():\n",
    "    for i in tqdm.tqdm(range(num_recon)):\n",
    "        # load the gt data as the input\n",
    "        obj = LayoutDataset.load_object(data_path + '/' + data_list[i] + '.json')\n",
    "        obj.to(device)\n",
    "\n",
    "        # feed through the encoder to get a code z\n",
    "        # root_code = encoder.encode_structure(obj)\n",
    "        root_code_and_kld = encoder.encode_structure(obj)\n",
    "        root_code = root_code_and_kld[:, :conf.feature_size]\n",
    "\n",
    "        # infer through the decoder to get the reconstructed output\n",
    "        # set maximal tree depth to conf.max_tree_depth\n",
    "        obj_arr = decoder.decode_structure(z=root_code, max_depth=conf.max_tree_depth)\n",
    "        obj_arr.get_arrbox()\n",
    "\n",
    "        # output the hierarchy\n",
    "        with open(os.path.join(recon_dir, data_list[i] + '_GT.txt'), 'w') as fout:\n",
    "            fout.write(str(obj)+'\\n\\n')\n",
    "            \n",
    "        with open(os.path.join(recon_dir, data_list[i] + '_PRED.txt'), 'w') as fout:\n",
    "            fout.write(str(obj_arr)+'\\n\\n')\n",
    "\n",
    "        # output the assembled box-shape\n",
    "        vis_utils.draw_partnet_objects([obj], \\\n",
    "            object_names=['GT'], leafs_only=True, \\\n",
    "            sem_colors_filename='./part_colors_magazine.txt', figsize=(5, 5), \\\n",
    "            out_fn=os.path.join(recon_dir, data_list[i] + '_GT.png'))\n",
    "        \n",
    "        vis_utils.draw_partnet_objects([obj], \\\n",
    "            object_names=['GT'], leafs_only=True, \\\n",
    "            sem_colors_filename='./part_colors_magazine.txt', figsize=(5, 5), \\\n",
    "            out_fn=os.path.join(recon_dir, data_list[i] + '_GT.svg'))\n",
    "        \n",
    "        vis_utils.draw_partnet_objects([obj_arr], \\\n",
    "            object_names=['PRED'], leafs_only=True, \\\n",
    "            sem_colors_filename='./part_colors_magazine.txt', figsize=(5, 5), \\\n",
    "            out_fn=os.path.join(recon_dir, data_list[i] + '_PRED.png'))\n",
    "\n",
    "        vis_utils.draw_partnet_objects([obj_arr], \\\n",
    "            object_names=['PRED'], leafs_only=True, \\\n",
    "            sem_colors_filename='./part_colors_magazine.txt', figsize=(5, 5), \\\n",
    "            out_fn=os.path.join(recon_dir, data_list[i] + '_PRED.svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:01<00:00,  4.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate shapes\n",
    "with torch.no_grad():\n",
    "    for i in tqdm.tqdm(range(num_gen)):\n",
    "        # get a Gaussian noise\n",
    "        code = torch.randn(1, conf.feature_size).to(device)\n",
    "        \n",
    "        # infer through the model to get the generated hierarchy\n",
    "        # set maximal tree depth to conf.max_tree_depth\n",
    "        obj_arr = decoder.decode_structure(z=code, max_depth=conf.max_tree_depth)\n",
    "\n",
    "        obj_arr.get_arrbox()\n",
    "        \n",
    "        # output the hierarchy\n",
    "        with open(os.path.join(gen_dir, 'gen-%03d.txt'%i), 'w') as fout:\n",
    "            fout.write(str(obj_arr)+'\\n\\n')\n",
    "\n",
    "        # output the assembled box-shape\n",
    "        vis_utils.draw_partnet_objects([obj_arr],\\\n",
    "                object_names=['GENERATION'], \\\n",
    "                figsize=(5, 5), out_fn=os.path.join(gen_dir, 'gen-%03d.png'%i),\\\n",
    "                leafs_only=True,sem_colors_filename='./part_colors_magazine.txt')\n",
    "        \n",
    "        vis_utils.draw_partnet_objects([obj_arr], \\\n",
    "            object_names=['GENERATION'], leafs_only=True, \\\n",
    "            sem_colors_filename='./part_colors_magazine.txt', figsize=(5, 5), \\\n",
    "            out_fn=os.path.join(gen_dir, 'gen-%03d.svg'%i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
